\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{listings} % For displaying code
\usepackage{algorithm2e} % pseudo-code

\def\rubric#1{\gre{Rubric: \{#1\}}}{}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\newcommand{\matCode}[1]{\lstinputlisting[language=Matlab]{a2f/#1.m}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}

\begin{document}


\title{Solutions for CPSC 340 Assignment 4}
\date{}
\maketitle


\underline{\textbf{Submitted by:}}\\ \\
 \textbf{Armaan Kaur Bajwa\\
 Student ID: 87921193\\ \\
 Sugun Machipeddy\\
 Student ID: 65753337} 

\vspace{1em}

\section*{Instructions}
\rubric{mechanics:3}

The above points are allocated for following the general homework instructions. In addition to the usual instructions,
\textbf{we have a NEW REQUIREMENT for this assignment} (and future assignments, unless it's a disaster):
if you're embedding your answers in a document that also contains the questions,
your answers should be in \blu{blue text}. This should hopefully make it much easier for the grader to find
your answers. To make something blue, you can use the LaTeX macro \verb|\blu{my text}|.


\section{Convex Functions}
\rubric{reasoning:5}


Show that the following functions are convex:

\enum{
\item $f(w) = \alpha w^2 - \beta w + \gamma$ with $w \in \R, \alpha \geq 0, \beta \in \R, \gamma \in \R$ (1D quadratic). \\ \\
\blu{Solution: \\ \\
$f'(w) = \frac{df(w)}{dw}$ \\ \\
$ = 2 \alpha w - \beta$ \\ \\
Now, $f''(w) = 2 \alpha$ \\ \\
Given, $\alpha \geq 0$ \\ \\
So, $f''(w) \geq 0$. \\ \\
Hence the given function is convex. }
\item $f(w) = w\log(w) $ with $w > 0$ (``neg-entropy'') \\ \\
\blu{Solution: \\ \\
$f'(w) = log(w) + w\frac{1}{w}$ \\ \\
$= log(w) + 1 $ \\ \\
Now, $f''(w) = \frac{1}{w}$ \\ \\
Given, $w > 0$ \\ \\
So, $\frac{1}{w} > 0$ \\ \\
So the function is convex.} \\
\item $f(w) = \norm{Xw-y}^2 + \lambda\norm{w}_1$ with $w \in \R^d, \lambda \geq 0$ (L1-regularized least squares). \\ \\
\blu{Solution: \\ \\
Taking $\norm{Xw-y}^2$, \\ \\
We know that a squared norm is a convex function. 
Also, if `$f$' is convex, then $f(Xw-y)$ is also convex. Hence, the squared norm of $Xw - y$ is convex. \\ \\
i.e.,  $\norm{Xw-y}^2$ is convex. \\ \\
Now, taking $\lambda\norm{w}_1$, \\ \\
$\norm{w}_1$ is convex, because norms are convex functions. \\ \\
Also, since $\lambda$ is  a constant which is greater than 0, so $\lambda\norm{w}_1$ will also be convex. \\ \\
Now, $f(w) = \norm{Xw-y}^2 + \lambda\norm{w}_1$ \\ \\
i.e. the sum of the above two convex functions. 
We know that the sum of convex functions is a convex function. So $f(w)$ is convex. } \\
\item $f(w) = \sum_{i=1}^n \log(1+\exp(-y_iw^Tx_i)) $ with $w \in \R^d$ (logistic regression). \\ \\
\blu{Solution: \\ \\
Replacing $y_iw^Tx_i$ by $z$ , we can write: \\ \\
$f(z) = \sum_{i=1}^n \log(1+\exp(-z))$ \\ \\
$ f'(z) = \sum_{i=1}^n \frac{- \exp(-z)}{1 + \exp(-z)}$ \\ \\
$ f''(z) = \sum_{i=1}^n \frac{\exp(-z)}{(1+ \exp(-z))^2} $ \\ \\
As we can see above, $f''(z) \geq 0$, so the given function is convex. \\
}
\item $f(w,w_0) = \sum_{i=1}^N[\max\{0,w_0 - w^Tx_i\} - w_0] + \frac{\lambda}{2}\norm{w}_2^2$  with $w \in R^d, w_0 \in \R, \lambda \geq 0$ (``1-class'' SVM). \\ \\
\blu{ Solution: \\ \\
\red{$\frac{\lambda}{2}\norm{w}_2^2$} is convex, because it is a norm multiplied by a constant. \\ \\
Now, $w_0 - w^Tx_i$ is linear, so it is convex. So \red{$\max\{0,w_0 - w^Tx_i\}$} is convex, because max of two convex functions is a convex.\\
From the above statements, \red{$\max\{0,w_0 - w^Tx_i\} - w_0$} is also convex, because it is a sum of two convex functions. \\
Hence, \red{$\sum_{i=1}^N[\max\{0,w_0 - w^Tx_i\} - w_0]$} is convex because it is summation of $n$ convex functions. \\
So, we can say that \red{$f(w,w_0) = \sum_{i=1}^N[\max\{0,w_0 - w^Tx_i\} - w_0] + \frac{\lambda}{2}\norm{w}_2^2$} is convex, again because it is sum of convex functions.
}
}



\section{Logistic Regression with Sparse Regularization}

If you run  \verb|python main.py -q 2|, it will:
\enum{
\item Load a binary classification dataset containing a training and a validation set.
\item `Standardize' the columns of $X$ and add a bias variable (in \emph{utils.load\_dataset}).
\item Apply the same transformation to $Xvalidate$ (in \emph{utils.load\_dataset}).
\item Fit a logistic regression model.
\item Report the number of features selected by the model (number of non-zero regression weights).
\item Report the error on the validation set.
}
Logistic regression does ok on this dataset,
but it uses all the features (even though only the prime-numbered features are relevant)
and the validation error is above the minimum achievable for this model
(which is 1 percent, if you have enough data and know which features are relevant).
In this question, you will modify this demo to use different forms of regularization
 to improve on these aspects.

Note: your results may vary a bit depending on versions of Python and its libraries.


\subsection{L2-Regularization}
\rubric{code:2}

Make a new class, \emph{logRegL2}, that takes an input parameter $\lambda$ and fits a logistic regression model with L2-regularization. Specifically, while \emph{logReg} computes $w$ by minimizing
\[
f(w) = \sum_{i=1}^n \log(1+\exp(-y_iw^Tx_i)),
\]
your new function \emph{logRegL2} should compute $w$ by minimizing
\[
f(w) = \sum_{i=1}^n \left[\log(1+\exp(-y_iw^Tx_i))\right] + \frac{\lambda}{2}\norm{w}^2.
\]
Hand in your updated code. Using this new code with $\lambda = 1$, report how the following quantities change: the training error, the validation error, the number of features used, and the number of gradient descent iterations.

Note: as you may have noticed, \texttt{lambda} is a special keyword in Python and therefore we can't use it as a variable name.
As an alternative I humbly suggest \texttt{lammy}, which is what my neice calls her stuffed animal toy lamb.
However, you are free to deviate from this suggestion. In fact, as of Python 3 one can now use actual greek letters as variable names, like the $\lambda$ symbol. But, depending on your text editor, it may be annoying to input this symbol. \\ \\
\blu{Solution: \\ \\
The code is available in linear\_model.py file \\
Logistic loss without regularization: \\ \\
logReg Training error 0.000 \\
logReg Validation error 0.084 \\ 
Number of features: 101 \\ 
Number of gradient descent iterations : 121 \\ \\
Logistic Loss with L2-Regularization: \\ \\ 
logRegL2 Training error 0.002 \\
logRegL2 Validation error 0.074 \\
Number of features: 101 \\
Number of gradient descent iterations : 36 \\ \\
With L2-Regularization the number of gradient descent iterations required to converge decreased from 121 to 36. The training error increased from 0 to 0.002 and there is a decrease in the validation error. There was no feature selection, both of them used all the 101 features. 
}

\subsection{L1-Regularization}
\rubric{code:3}

Make a new class, \emph{logRegL1}, that takes an input parameter $\lambda$ and fits a logistic regression model with L1-regularization,
\[
f(w) = \sum_{i=1}^n \left[\log(1+\exp(-y_iw^Tx_i))\right] + \lambda\norm{w}_1.
\]
Hand in your updated code. Using this new code with $\lambda = 1$, report how the following quantities change: the training error, the validation error, the number of features used, and the number of gradient descent iterations.


You should use the function \emph{minimizers.findMinL1}, which implements a
proximal-gradient method to minimize the sum of a differentiable function $g$ and $\lambda\norm{w}_1$,
\[
f(w) = g(w) + \lambda \norm{w}_1.
\]
This function has a similar interface to \emph{findMin}, \textbf{EXCEPT} that (a) you
only pass in the the function/gradient of the differentiable
part, $g$, rather than the whole function $f$; and (b) you need to provide the value $\lambda$.
Thus, your \texttt{funObj} shouldn't actually contain the L1 regularization, since it's implied
in the way you express your objective to the optimizer. \\ \\
\blu{Solution: \\ \\
The code is available in linear\_model.py file  \\
Logistic loss without L1-Regularization: \\ \\
logRegL1 Training error 0.000 \\
logRegL1 Validation error 0.048 \\
Number of features selected: 72 \\
Number of gradient descent iterations : 351\\ \\
In Comparision with No Regression, the training error remains the same i.e 0.0, the validation error decreases and there is better feature selection. The nummber of features selected are 72 in comparision to 101. L1- Regualrization falls behind in terms of the  numebr of gradient descent iterations required to converge, it took 351 itearations.
}
\subsection{L0-Regularization}
\rubric{code:4}

The class \emph{logRegL0} contains part of the code needed to implement the \emph{forward selection} algorithm,
which approximates the solution with L0-regularization,
\[
f(w) =  \sum_{i=1}^n \left[\log(1+\exp(-y_iw^Tx_i))\right] + \lambda\norm{w}_0.
\]
The \texttt{for} loop in this function is missing the part where we fit the model using the subset \emph{selected\_new},
then compute the score and updates the \emph{minLoss/bestFeature}.
Modify the \texttt{for} loop in this code so that it fits the model using only
the features \emph{selected\_new}, computes the score above using these features,
and updates the \emph{minLoss/bestFeature} variables.
\blu{Hand in your updated code. Using this new code with $\lambda=1$,
report the training error, validation error, and number of features selected.}

Note that the code differs a bit from what we discussed in class,
since we assume that the first feature is the bias variable and assume that the
bias variable is always included. Also, note that for this particular case using
the L0-norm with $\lambda=1$ is equivalent to what is known as the Akaike
Information Criterion (AIC) for variable selection. \\ \\
\blu{Solution: \\ \\
The code is available in linear\_model.py file  \\
Logistic loss without L0-Regularization: \\ \\
Training error 0.000 \\
Validation error 0.020 \\
Number of fetures selected: 26 \\
Number of gradient descent iterations : 179 \\
}

\subsection{Discussion}
\rubric{reasoning:2}

In a short paragraph, briefly discuss your results from the above. How do the
different forms of regularization compare with each other?
Can you provide some intuition for your results? No need to write a long essay, please! \\ \\
\blu { Ans:
L2-Regialrization prevents over fittng and that is why it has a non zero training error. Its training error is lower than standard logistic regression and also uses all the features. \\
L1-Regularization has a training error of zero as a feature is not selected if the error is too large. It also has the lowest validation error among all regularizations. The classifier selected 72 features among the 101 features. It is also slow as we can see from the number of gradient descent iterations \\
L0-Regularization uses forward selection to select a subset of features. it is faster than L1, but slower than L2-Regularization. the classifier selected 26 features in my case.
}


\subsection{Comparison with scikit-learn}
\rubric{reasoning:1}

Compare your results (training error, validation error, number of nonzero weights) for L2 and L1 regularization with scikit-learn's LogisticRegression. Use the
\texttt{penalty} parameter to specify the type of regularization. The parameter \texttt{C} corresponds to $\frac{1}{\lambda}$, so if
you had $\lambda=1$ then use \texttt{C=1} (which happens to be the default anyway).
You should set \texttt{fit\string_intercept} to \texttt{False} since we've already added the column of ones to $X$ and thus
there's no need to explicitly fit an intercept parameter. After you've trained the model, you can access the weights
with \texttt{model.coef\string_}. \\ \\
\blu{Solution: \\ \\
The code is available in linear\_model.py file  \\
Scikit learns L2-Regularization \\ \\
Training error 0.002 \\
Validation error 0.074 \\
Number of fetures selected: 101 \\ \\
our implemetnation of L2-Regularization \\ \\
logRegL1 Training error 0.002 \\
logRegL1 Validation error 0.074 \\
Number of fetures selected: 101 \\ \\
Scikit learns L1-Regularization \\ \\ 
Training error 0.000 \\
Validation error 0.052 \\
Number of fetures selected: 71 \\ \\
 our implemetnation of L1-Regularization \\ \\
logRegL1 Training error 0.000 \\
logRegL1 Validation error 0.048 \\
Number of fetures selected: 72 \\
}

\section{Multi-Class Logistic}

If you run \verb|python main.py -q 3| the code loads a multi-class
classification dataset with $y_i \in \{0,1,2,3,4\}$ and fits a `one-vs-all' classification
model using least squares, then reports the validation error and shows a plot of the data/classifier.
The performance on the validation set is ok, but could be much better.
For example, this classifier never even predicts that examples will be in classes 0 or 4.


\subsection{Softmax Classification, toy example}
\rubric{reasoning:2}

Linear classifiers make their decisions by finding the class label $c$ maximizing the quantity $w_c^Tx_i$, so we want to train the model to make $w_{y_i}^Tx_i$ larger than $w_{c'}^Tx_i$ for all the classes $c'$ that are not $y_i$. 
Here $c'$ is a possible label and $w_{c'}$ is row $c'$ of $W$. Similarly, $y_i$ is the training label, $w_{y_i}$ is row $y_i$ of $W$, and in this setting we are assuming a discrete label $y_i \in \{1,2,\dots,k\}$. Before we move on to implementing the softmax classifier to fix the issues raised in the introduction, let's work through a toy example:

Consider the dataset below, which has $n=10$ training examples, $d=2$ features, and $k=3$ classes:
\[
X = \begin{bmatrix}0 & 1\\1 & 0\\ 1 & 0\\ 1 & 1\\ 1 & 1\\ 0 & 0\\  1 & 0\\  1 & 0\\  1 & 1\\  1 &0\end{bmatrix}, \quad y = \begin{bmatrix}1\\1\\1\\2\\2\\2\\3\\3\\3\\3\end{bmatrix}.
\]
Suppose that you want to classify the following test example:
\[
\hat{x} = \begin{bmatrix}1 & 1\end{bmatrix}.
\]
Suppose we fit a multi-class linear classifier using the softmax loss, and we obtain the following weight matrix:
\[
W =
\begin{bmatrix}
+2 & -1\\
+2 & +2\\
+3 & -1
\end{bmatrix}
\]
\blu{Under this model, what class label would we assign to the test example? (Show your work.)} \\ \\\
\blu{Solution: \\ \\
$\hat{x}.(w1.T) = 1*2 - 1 *1 = 1$ \\
$\hat{x}.(w2.T)  = 2*1 + 2 *1 = 4$ \\
$\hat{x}.(w3.T) = 3*1 - 1*1 = 2$ \\
since the score of $\hat{x}.(w2.T)$ is the highest . the class label to be assigned is '2'.
}

\subsection{One-vs-all Logistic Regression}
\rubric{code:2}

Using the squared error on this problem hurts performance because it has `bad errors' (the model gets penalized if it classifies examples `too correctly').
Write a new class, \emph{logLinearClassifier}, that replaces the squared loss in the one-vs-all model with the logistic loss. \blu{Hand in the code and report the validation error}. \\ \\ 
\blu{Solution: \\ \\
One-vs-all Logistic Regularization \\ \\
The code is available in linear\_model.py file \\
logLinearClassifier Training error 0.084\\
logLinearClassifier Validation error  0.070
}
\subsection{Softmax Classifier Implementation}
\rubric{code:5}

Using a one-vs-all classifier hurts performance because the classifiers are fit independently, so there is no attempt to calibrate the columns of the matrix $W$. An alternative to this independent model is to use the softmax loss, which is given by
\[
f(W) = \sum_{i=1}^n \left[-w_{y_i}^Tx_i + \log\left(\sum_{c' = 1}^k \exp(w_{c'}^Tx_i)\right)\right] \, ,
\]

The partial derivatives of this function, which make up its gradient, are given by

\[
\frac{\partial f}{\partial W_{cj}} = \sum_{i=1}^n x_{ij}[p(y_i=c | W,x_i) - I(y_i = c)] \, ,
\]
where... 
\begin{itemize}
\item $I(y_i = c)$ is the indicator function (it is $1$ when $y_i=c$ and $0$ otherwise)
\item $p(y_i=c | W, x_i)$ is the predicted probability of example $i$ being class $c$, defined as
\[
p(y_i | W, x_i) = \frac{\exp(w_{y_i}^Tx_i)}{\sum_{c'=1}^k\exp(w_{c'}^Tx_i)}
\]

\end{itemize}


(Good news: in previous offerings of CPSC 340, you had to derive this! I think you've probably taken enough derivatives by now though.)

Make a new class, \emph{softmaxClassifier}, which fits $W$ using the softmax loss from the previous section instead of fitting $k$ independent classifiers. \blu{Hand in the code and report the validation error}.

Hint: you may want to use \verb|utils.check_gradient| to check that your implementation of the gradient is correct. \\ \\
\blu{Solution: \\ \\
Softmax classifier implementation \\ \\
The code is available in linear\_model.py file \\
Training error 0.000 \\
Validation error 0.008 
}


\subsection{Comparison with scikit-learn, again}
\rubric{reasoning:1}

Compare your results (training error and validation error for both one-vs-all and softmax) with scikit-learn's \texttt{LogisticRegression}, 
which can also handle multi-class problems. 
One-vs-all is the default; for softmax, set \texttt{multi\string_class='multinomial'}. For the softmax case,
you'll also need to change the solver. You can use \texttt{solver='lbfgs'}.
Since your comparison code above isn't using regularization, set \texttt{C} very large to effectively disable regularization. 
Again, set \texttt{fit\string_intercept} to \texttt{False} for the same reason as above. \\ \\
\blu{Solution: \\ \\
The code is available in linear\_model.py file \\
Scikit learns one vs all implementation \\
Training error 0.084 \\  
Validation error 0.070 \\ \\
our one vs all implementation \\
logLinearClassifier Training error 0.084 \\
logLinearClassifier Validation error 0.070 \\ \\
Scikit learns softmax implementation \\
Training error 0.000 \\ 
Validation error 0.012 \\  \\
our softmax implementation \\ 
Training error 0.000 \\
Validation error 0.008
}

\subsection{Cost of Multinomial Logistic Regression}
\rubric{reasoning:2}

Assuming that we have
\items{
\item $n$ training examples.
\item $d$ features.
\item $k$ classes.
\item $t$ testing examples.
\item $T$ iterations of gradient descent for training.
}
\enum{
\item In Big-O notation, what is the cost of training the softmax classifier? \\
\blu{Answer: $O(ndkT)$}
\item What is the cost of classifying the test examples? \\
\blu{Answer: $O(tdk)$}
}




\section{Very-Short Answer Questions}
\rubric{reasoning:9}

\enum{
\item Why would you use a score BIC instead of a validation error for feature selection?\\
\blu{Answer: Because as 'n' goes to infinity, BIC recovers the true model as it penalizes more for a large number of objects, whereas optimizing too many times for validation error might include optimization bias.}
\item Why do we use forward selection instead of exhaustively search all subsets in search and score methods?\\
\blu{Answer: Because using forward selection is comparatively cheaper, overfits less and has fewer false positives. }
\item In L2-regularization, how does $\lambda$ relate to the two parts of the fundamental trade-off? \\
\blu{Answer: In L2-regularization, as $\lambda$ increases, the training error increases and the approximation error decreases. } 
\item Give one reason why one might chose to use L1 regularization over L2 and give one reason for the reverse case.\\
\blu{Answer: We should choose L1 regularization over L2 when we need to absolutely know which features are selected during feature selection, because in L1-regularization makes some 'w' equal to zero for the features which are not required. \\ 
We choose L2-regularization over L1 in case of non-unique solution.}
\item What is the main problem with using least squares to fit a linear model for binary classification? \\
\blu{Answer: With binary classification, if you use squared error, you may get penalized for being 'too right'.}
\item For a linearly separable binary classification problem, how does a linear SVM differ from a classifier found using the perceptron algorithm? \\
\blu{Answer: For a classifier found via Perceptron algorithm, all the training examples contribute to the loss and hence to the value of $w$, whereas, in case of SVM, if something is not a support vector, i.e. $ y_iw^Tx_i \geq 0$ then its loss is zero and it "doesn't contribute". }
\item Which of the following methods produce linear classifiers? (a) binary least squares as in Question 3, (b) the perceptron algorithm, (c) SVMs, and (d) logistic regression. \\
\blu{Answer: All of them.}
\item What is the difference between multi-label and multi-class classification? \\
\blu{Answer: In multi-label problems it is possible to have more than one 'correct' class label, whereas in multi-class, it is one vs all, i.e. only one correct class label is allowed.}
\item Fill in the question marks: for one-vs-all multi-class logistic regression, we are solving $??$ optimization problem(s) of dimension $??$. On the other hand, for softmax logistic regression, we are solving $??$ optimization problem(s) of dimension $??$. \\
\blu{Answer:} for one-vs-all multi-class logistic regression, we are solving \blu{k, the number of class labels} optimization problem(s) of dimension $\blu{d *1}$. On the other hand, for softmax logistic regression, we are solving $\blu{'1'}$ optimization problem(s) of dimension $\blu{d * k}$.
}



\end{document}
