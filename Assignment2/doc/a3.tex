\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{listings} 
\usepackage{algorithm2e}

\def\rubric#1{\gre{Rubric: \{#1\}}}{}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\newcommand{\matCode}[1]{\lstinputlisting[language=Matlab]{a2f/#1.m}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}

\begin{document}

\title{Solutions to CPSC 340 Assignment 3}
\date{}
\maketitle

\vspace{-2em}

\underline{\textbf{Submitted by:}}\\ \\
 \textbf{Armaan Kaur Bajwa\\
 Student ID: 87921193\\ \\
 Sugun Machipeddy\\
 Student ID: 65753337} 

\section*{Instructions}
\rubric{mechanics:3}

The above points are allocated for following the general homework instructions on the course homepage.

\section{Vectors, Matrices, and Quadratic Functions}

\subsection{Basic Operations}
\rubric{reasoning:3}

\noindent Using the definitions below,
\[
\alpha = 5,\quad
x = \left[\begin{array}{c}
2\\
-3\\
\end{array}\right], \quad
y = \left[\begin{array}{c}
1\\
4\\
\end{array}\right],\quad
z = \left[\begin{array}{c}
2\\
0\\
1\end{array}\right],
\quad
A = \left[\begin{array}{cc}
1 & 2\\
2 & 3\\
3 & -2
\end{array}\right],
\]
\blu{evaluate the following expressions} (show your work, but you may use answers from previous parts to simplify calculations):\\
\enum{
\item $x^Tx = \left[\begin{array}{cc}
2 & -3
\end{array}\right]*\left[\begin{array}{c}
2\\
-3\\
\end{array}\right] = 2*2 + (-3)*(-3) = 13$
\item $\norm{x}^2 = (2)^2 + (-3)^2 = 4+9 = 13$
\item $x^T(x + \alpha y) = \left[\begin{array}{cc}
2 & -3
\end{array}\right] * (\left[\begin{array}{c}
2\\
-3\\
\end{array}\right] + 5*\left[\begin{array}{c}
1\\
4\\
\end{array}\right]) \\ \\ = \left[\begin{array}{cc}
2 & -3
\end{array}\right] * (\left[\begin{array}{c}
2\\
-3\\
\end{array}\right] + \left[\begin{array}{c}
5\\
20\\
\end{array}\right]) \\ \\= \left[\begin{array}{cc}
2 & -3
\end{array}\right] * (\left[\begin{array}{c}
7\\
17\\
\end{array}\right] ) \\ \\ = 2*7 + (-3)*17 = 14 - 51 \\ \\ = -37$
\item $Ax = \left[\begin{array}{cc}
1 & 2\\
2 & 3\\
3 & -2
\end{array}\right] * \left[\begin{array}{c}
2\\
-3\\
\end{array}\right] \\ \\
= \left[\begin{array}{cc}
1*2 + 2*(-3)\\
2*2 + 3*(-3)\\
3*2 + -2*(-3)
\end{array}\right] \\ \\ 
=\left[\begin{array}{cc}
2 - 6\\
4 - 9\\
6 + 6
\end{array}\right] \\ \\
=\left[\begin{array}{c}
-4\\
-5\\
12
\end{array}\right]$
\item $z^TAx = \left[\begin{array}{ccc}
2 & 0 & 1
\end{array}\right] * \left[\begin{array}{c}
-4\\
-5\\
12
\end{array}\right] \\ \\= 2*(-4) + 0*(-5) + 1*12 \\= -8 + 0 +12 \\= 4$
\item $A^TA = \left[\begin{array}{ccc}
1 & 2 & 3\\
2 & 3 & -2
\end{array}\right] * \left[\begin{array}{cc}
1 & 2\\
2 & 3\\
3 & -2
\end{array}\right] \\ \\= \left[\begin{array}{cc}
1*1+2*2+3*3 & 1*2+2*3+3*(-2) \\
2*1+3*2+(-2)*3 & 2*2+3*3+(-2)*(-2) 
\end{array}\right] \\ \\= \left[\begin{array}{cc}
1+4+9 & 2+6-6 \\
2+6-6 & 4+9+4
\end{array}\right] \\ \\
= \left[\begin{array}{cc}
14 & 2 \\
2 & 17
\end{array}\right]$
}

If $\{\alpha,\beta\}$ are scalars, $\{x,y,z\}$ are real-valued column-vectors of length $d$,
and $\{A,B,C\}$ are real-valued $d\times d$ matrices, \blu{state whether each of the below statements is true or false in general
and give a short explanation.}
\enum{
\addtocounter{enumi}{6}
\item $yy^Ty = \norm{y}^2y$ \blu{True} \\because $yy^Ty = y*\norm{y}^2$ , and $\norm{y}^2$ is a scalar so the order of its multiplication with vector $y$ will not make a difference.
\item $x^TA^T(Ay + Az) = x^TA^TAy + z^TA^TAx$ \blu{True} \\because $a^TAb = b^TA^Ta$, by which $x^TA^TAz = z^TA^TAx$
\item $x^T(B + C) = Bx + Cx$ \blu{False} \\
Because dimensions of $x^TB$ are $1\times d$, whereas the dimensions of $Bx$ will be $d\times1$, which does not match. 
\item $(A + BC)^T = A^T + C^TB^T$ \blu{True}\\
Because $(A + BC)^T = A^T + (BC)^T = A^T+C^TB^T$
\item $(x-y)^T(x-y) = \norm{x}^2 - x^Ty + \norm{y}^2$ \blu{False} \\
Because $(x-y)^T(x-y) = (x^T - y^T)(x-y) = x^Tx - x^Ty - y^Tx +y^Ty \\= \norm{x}^2 - x^Ty - y^Tx + \norm{y}^2$
\item $(x-y)^T(x+y) = \norm{x}^2 - \norm{y}^2$ \blu{True} \\ 
Because $(x-y)^T(x+y) = (x^T-y^T)(x+y) \\= x^Tx + x^Ty - y^Tx + y^T \\= \norm{x}^2 - \norm{y}^2$ because $x^Ty = y^Tx$
}

\subsection{Converting to Matrix/Vector/Norm Notation}
\rubric{reasoning:2}

Using our standard supervised learning notation ($X$, $y$, $w$)
express the following functions in terms of vectors, matrices, and norms (there should be no summations or maximums).
\enum{
\item $\sum_{i=1}^n |w^Tx_i - y_i|$ \\ \\
\blu{Answer:}  $\left \| Xw^T - y \right \|_{1}$
\item $\max_{i \in \{1,2,\dots,n\}} |w^Tx_i  - y_i| + \frac{\lambda}{2}\sum_{j=1}^d w_j^2 $  \\ \\
\blu{Answer:} $ \left \| Xw^T - y \right \|_{\infty} + \frac{\lambda}{2}\norm{w}^2 $
\item $\sum_{i=1}^n z_i (w^Tx_i - y_i)^2 + \lambda \sum_{j=1}^{d} |w_j|$ \\ \\
\blu{Answer:} $(Xw - y)^TZ(Xw -y) + \lambda \left \| w \right \|_{1}$ 
}
You can use $Z$ to denote a diagonal matrix that has the values $z_i$ along the diagonal.


\subsection{Minimizing Quadratic Functions as Linear Systems}
\rubric{reasoning:3}

Write finding a minimizer $w$ of the functions below as a system of linear equations (using vector/matrix notation and simplifying as much as possible). Note that all the functions below are convex  so finding a $w$ with $\nabla f(w) = 0$ is sufficient to minimize the functions (but show your work in getting to this point).
\enum{
\item $f(w) = \frac{1}{2}\norm{w-v}^2$\\ \\
\blu{Answer:} $ f(w) = \frac{1}{2}(w-v)^T(w-v)$ \\ \\
$ \nabla f(w) = \frac{1}{2}(w-v) + \frac{1}{2}(w-v)$ \\ \\
$ \nabla f(w) = w-v$\\ \\ 
Putting $\nabla f(w) = 0$, we get:\\ \\
$w-v=0$\\ \\
$w=v$ \\
\item $f(w) = \frac{1}{2}\norm{w}^2 + w^TX^Ty$ \\ \\
\blu{Answer:} $\nabla f(w) = 2w + X^Ty $ \\ \\
Putting $\nabla f(w) = 0$, we get: \\ \\
$ 2w + X^Ty = 0$ \\ \\
$w = -\frac{1}{2}X^Ty$ \\ \\
\item $f(w) = \frac{1}{2}\sum_{i=1}^n z_i (w^Tx_i - y_i)^2$ \\ \\
\blu{Answer:} $\nabla f(w) = X^TzXw - X^Tzy$ \\ \\
Putting $\nabla f(w) = 0$, we get: \\ \\
$X^TzXw - X^Tzy = 0$ \\ \\
$X^TzXw  = X^Tzy$ \\ \\
the minimizers of this objectve function are the solutions to this linear equation.
}


\section{Robust Regression and Gradient Descent}

If you run \verb|python main.py -q 2|, it will load a one-dimensional regression
dataset that has a non-trivial number of `outlier' data points.
These points do not fit the general trend of the rest of the data,
and pull the least squares model away from the main downward trend that most data points exhibit:
\centerfig{.7}{../figs/least_squares_outliers.pdf}

Note: we are fitting the regression without an intercept here, just for simplicity of the homework question.
In reality one would rarely do this. But here it's OK because the ``true'' line 
passes through the origin (by design). In Q3.1 we'll address this explicitly.

\subsection{Weighted Least Squares in One Dimension}
\rubric{code:3}

One of the most common variations on least squares is \emph{weighted} least squares. In this formulation, we have a weight $z_i$ for every training example. To fit the model, we minimize the weighted squared error,
\[
f(w) =  \frac{1}{2}\sum_{i=1}^n z_i(w^Tx_i - y_i)^2.
\]
In this formulation, the model focuses on making the error small for examples $i$ where $z_i$ is high. Similarly, if $z_i$ is low then the model allows a larger error.

Complete the model class, \emph{WeightedLeastSquares}, that implements this model
(note that Q1.3.3 asks you to show how this formulation can be solved as a linear system).
Apply this model to the data containing outliers, setting $z = 1$ for the first
$400$ data points and $z = 0.1$ for the last $100$ data points (which are the outliers).
\blu{Hand in your code and the updated plot}.

\blu{Answer:} The code is available in linear\_model.py. With weighted least squares, there is better fitting of the points. The training error is 40.9.\\
\centerfig{.7}{../figs/least_squares_outliers_weighted.pdf}

\subsection{Smooth Approximation to the L1-Norm}
\rubric{reasoning:3}

Unfortunately, we typically do not know the identities of the outliers. In situations where we suspect that there are outliers, but we do not know which examples are outliers, it makes sense to use a loss function that is more robust to outliers. In class, we discussed using the sum of absolute values objective,
\[
f(w) = \sum_{i=1}^n |w^Tx_i - y_i|.
\]
This is less sensitive to outliers than least squares, but it is non-differentiable and harder to optimize. Nevertheless, there are various smooth approximations to the absolute value function that are easy to optimize. One possible approximation is to use the log-sum-exp approximation of the max function\footnote{Other possibilities are the Huber loss, or $|r|\approx \sqrt{r^2+\epsilon}$ for some small $\epsilon$.}:
\[
|r| = \max\{r, -r\} \approx \log(\exp(r) + \exp(-r)).
\]
Using this approximation, we obtain an objective of the form
\[
f(w) {=} \sum_{i=1}^n  \log\left(\exp(w^Tx_i - y_i) + \exp(y_i - w^Tx_i)\right).
\]
which is smooth but less sensitive to outliers than the squared error. \blu{Derive
 the gradient $\nabla f$ of this function with respect to $w$. You should show your work but you do \underline{not} have to express the final result in matrix notation.}

\blu{Answer:} $\nabla f = \frac{\partial( \sum_{i=1}^n  \log\left(\exp(w^Tx_i - y_i) + \exp(y_i - w^Tx_i)\right )}{\partial w}$ \\ \\
 $\nabla f = \sum_{i=1}^n \frac{\frac{\partial(\exp(w^Tx_i - y_i) + \exp(y_i - w^Tx_i)}{\partial w_i}}{\exp(w^Tx_i - y_i) + \exp(y_i - w^Tx_i)}$ \\ \\
 $\nabla f =\sum_{i=1}^n \frac{(\exp(w^Tx_i - y_i) - \exp(y_i - w^Tx_i))x_i}{\exp(w^Tx_i - y_i) + \exp(y_i - w^Tx_i)} $

\subsection{Robust Regression}
\rubric{code:2,reasoning:1}

The class \emph{LinearModelGradient} is the same as \emph{LeastSquares}, except that it fits the least squares model using a gradient descent method. If you run \verb|python main.py -q 2.3| you'll see it produces the same fit as we obtained using the normal equations.

The typical input to a gradient method is a function that, given $w$, returns $f(w)$ and $\nabla f(w)$. See \emph{funObj} in \emph{LinearModelGradient} for an example. Note that the \emph{fit} function of \emph{LinearModelGradient} also has a numerical check that the gradient code is approximately correct, since implementing gradients is often error-prone.\footnote{Sometimes the numerical gradient checker itself can be wrong. See CPSC 303 for a lot more on numerical differentiation.}

An advantage of gradient-based strategies is that they are able to solve
problems that do not have closed-form solutions, such as the formulation from the
previous section. The class \emph{LinearModelGradient} has most of the implementation
of a gradient-based strategy for fitting the robust regression model under the log-sum-exp approximation.
The only part missing is the function and gradient calculation inside the \emph{funObj} code.
\blu{Modify \emph{funObj} to implement the objective function and gradient based on the smooth
approximation to the absolute value function (from the previous section). Hand in your code, as well
as the plot obtained using this robust regression approach.}

\blu{Answer:} The code is available in linear\_model.py. the objective function and its gradient have been implemented in \emph{funObj} method. The training error is 40.6. \\
\centerfig{.7}{../figs/least_squares_robust.pdf}

\section{Linear Regression and Nonlinear Bases}

In class we discussed fitting a linear regression model by minimizing the squared error.
In this question, you will start with a data set where least squares performs poorly.
You will then explore how adding a bias variable and using nonlinear (polynomial) bases can drastically improve the performance.
You will also explore how the complexity of a basis affects both the training error and the test error.
In the final part of the question, it will be up to you to design a basis with better performance than polynomial bases.

\subsection{Adding a Bias Variable}
\rubric{code:3,reasoning:1}

If you run  \verb|python main.py -q 3|, it will:
\enum{
\item Load a one-dimensional regression dataset.
\item Fit a least-squares linear regression model.
\item Report the training error.
\item Report the test error (on a dataset not used for training).
\item Draw a figure showing the training data and what the linear model looks like.
}
Unfortunately, this is an awful model of the data. The average squared training error on the data set is over 28000
(as is the test error), and the figure produced by the demo confirms that the predictions are usually nowhere near
 the training data:
\centerfig{.5}{../figs/least_squares_no_bias.pdf}
The $y$-intercept of this data is clearly not zero (it looks like it's closer to $200$),
so we should expect to improve performance by adding a \emph{bias} (a.k.a. intercept) variable, so that our model is
\[
y_i = w^Tx_i + w_0.
\]
instead of
\[
y_i = w^Tx_i.
\]
\blu{In file \emph{linear\_model.py}, complete the class, \emph{LeastSquaresBias}, 
that has the same input/model/predict format as the \emph{LeastSquares} class, 
but that adds a \emph{bias} variable (also called an intercept) $w_0$ (also called $\beta$ in lecture). Hand in your new class, the updated plot, 
and the updated training/test error.}

Hint: recall that adding a bias $w_0$ is equivalent to adding a column of ones to the matrix $X$. Don't forget that you need to do the same transformation in the \emph{predict} function.

\blu{Answer:} The code has been implemented in \emph{LeastSquaresBias} class in \emph{linear\_model.py}. It uses the same input/model/predict format as least squares except it adds bias. There is significant improvement in training and testing error without bias. The errors obtained are: \\ \\
Training error = 3551.3
\newline Test error     = 3393.9
\newline the bias obtained is 157.093
\centerfig{.7}{../figs/least_squares_bias.pdf}


\subsection{Polynomial Basis}
\rubric{code:4,reasoning:1}

Adding a bias variable improves the prediction substantially, but the model is still problematic because the target seems to be a \emph{non-linear} function of the input.
Complete \emph{LeastSquarePoly} class, that takes a data vector $x$ (i.e., assuming we only have one feature) and the polynomial order $p$. The function should perform a least squares fit based on a matrix $Z$ where each of its rows contains the values $(x_{i})^j$ for $j=0$ up to $p$. E.g., \emph{LeastSquaresPoly.fit(x,y)}  with $p = 3$ should form the matrix
\[
Z =
\left[\begin{array}{cccc}
1 & x_1 & (x_1)^2 & (x_1)^3\\
1 & x_2 & (x_2)^2 & (x_2)^3\\
\vdots\\
1 & x_n & (x_n)^2 & (x_N)^3\\
\end{array}
\right],
\]
and fit a least squares model based on it.
\blu{Hand in the new class, and report the training and test error for $p = 0$ through $p= 10$. Explain the effect of $p$ on the training error and on the test error.}

Note: you should write the code yourself; don't use a library like sklearn's \texttt{PolynomialFeatures}.

\blu{Answer:} The code is available in \emph{LeastSquaresPoly} class in \emph{linear\_model.py} file. The training and test error decreases as the degree of the polynomial increases. But it starts to overfit the data with increasing $p$ which is not desirable. The errors obtained as we go from $p = 0 $ to $p = 10 $ are: \\
\newline p = 0
\newline Training error = 15480.5
\newline Test error     = 14390.8 \\ 

p = 1
\newline Training error = 3551.3
\newline Test error     = 3393.9 \\ 

p = 2
\newline Training error = 2168.0
\newline Test error     = 2480.7 \\

p = 3
\newline Training error = 252.0
\newline Test error     = 242.8 \\

p = 4
\newline Training error = 251.5
\newline Test error     = 242.1 \\ 

p = 5
\newline Training error = 251.1
\newline Test error     = 239.5 \\ 

p = 6
\newline Training error = 248.6
\newline Test error     = 246.0 \\ 

p = 7
\newline Training error = 247.0
\newline Test error     = 242.9 \\ 

p = 8
\newline Training error = 241.3
\newline Test error     = 246.0 \\ 

p = 9
\newline Training error = 235.8
\newline Test error     = 259.3 \\ 

p = 10
\newline Training error = 235.1
\newline Test error     = 256.3 \\ 

\centerfig{.7}{../figs/PolyBasis1.pdf}
\centerfig{.7}{../figs/PolyBasis7.pdf}
\centerfig{.7}{../figs/PolyBasis10.pdf}

\section{Non-Parametric Bases and Cross-Validation}

Unfortunately, in practice we often don't know what basis to use.
However, if we have enough data then we can make up for this by using a basis that is flexible enough to
model any reasonable function. These may perform poorly if we don't have much data, but can
 perform almost as well as the optimal basis as the size of the dataset grows.
Using the same data set as in the previous question, 
in this question you will explore using Gaussian radial basis functions (RBFs),
 which have this property. These RBFs depend on a hyperparameter $\sigma$, which
 (like $p$ in the polynomial basis) can be chosen using a validation set.
 In this question, you will also see how cross-validation allows you to tune
 parameters of the model on a larger dataset than a strict training/validation split would allow.


\subsection{Proper Training and Validation Sets}
\rubric{reasoning:3}

If you run \verb|python main.py -q 4|, it will load the dataset and split the training examples
 into ``train'' and ``validation'' sets. It will then search for the best value of $\sigma$
 for the RBF basis.\footnote{if you look at the code you'll see that it also uses a tiny bit of regularization since $Z^TZ$ tends to be very close to singular; more on this later in the course.}
  Once it has the ``best" value of $\sigma$, it re-trains on the entire dataset and reports the
  training error on the full training set as well as the error on the test set.

Unfortunately, there is a problem with the way this is done: the data isn't shuffled before being split.
As a result, the IID assumption is badly broken and we end up with poor test error. Here is the plot:

\centerfig{.7}{../figs/least_squares_rbf_bad.pdf}


\blu{Fix the problem by either randomizing the split yourself or using \texttt{sklearn.model\_selection.train\_test\_split} with \texttt{train\_size=0.5}. Compare the train/test errors and plot before vs. after fixing this problem.}

\blu{Answer:} The data was randomized before splitting and this improved the test error. 
\newline Before randomizing, the errors obtained are:
\newline Training error = 2184.1
\newline Test error     = 2495.9
\newline  Value of sigma that achieved the lowest validation error = 64. \\
\newline After randomizing, the errors obtained are:
\newline Training error = 203.9
\newline Test error     = 332.0
\newline Value of sigma that achieved the lowest validation error = 2. \\
\centerfig{.7}{../figs/least_squares_rbf_bad.pdf}
\centerfig{.7}{../figs/least_squares_rbf_randomization .pdf}


\subsection{Cross-Validation}
\rubric{code:3,reasoning:1}

Now that we've dealt with the randomization, something's still a bit disturbing:
if you run the script more than once it might choose different values of $\sigma$.
This variability would be reduced if we
had a larger ``train'' and ``validation'' set, and one way to simulate this is
with \emph{cross-validation}. 

\enum{
\item What are two different ``best'' values of $\sigma$ you've observed after re-running the code a few times? (Not all students will have the same answer here; that's OK.) \\
\newline \blu{Answer:} The two best values of $\sigma$ observed after re-running the code were $\sigma = 2$ and $\sigma =16$ .
\centerfig{.7}{../figs/least_squares_rbf_good3.pdf}
\centerfig{.7}{../figs/least_squares_rbf_good2.pdf}
\item Implement 10-fold cross-validation to select $\sigma$, and hand in your code. What value of $\sigma$ does this procedure typically select? \\
\newline \blu{Answer:} The implementation for 10-fold cross validation is available in \emph {main.py} file. The value of $\sigma$ that  the procedure typically selects is either $\sigma = 1$ or $\sigma = 0.5$
\centerfig{.7}{../figs/least_squares_rbf_10_fold.pdf}
}


\subsection{Cost of Non-Parametric Bases}
\rubric{reasoning:3}

When dealing with larger datasets, an important issue is the dependence of the
computational cost on the number of training examples $n$ and the number of
features $d$. \\ \\
What is the cost in big-O notation of training the model on $n$ training examples with $d$ features under (a) the linear basis and (b) Gaussian RBFs (for a fixed $\sigma$)? \\
\blu{Answer:} The cost to train the model under the linear basis is $O(nd^2 + d^3)$. The cost of training the model under Gaussian RBFs is $O(n^2d + n^3)$. \\ \\
What is the cost of classifying $t$ new examples under each of these two bases? \\
\blu{Answer:} The cost of classifying $t$ new examples under linear basis is $O(dt)$, while the cost of the same under Gaussian RBFs is $O(n^2d + n^3)$. \\ \\
When are RBFs cheaper to train? \\
\blu{Answer:} From above, we can see that RBFs are easy to train when the value of $d$ is larger than that of $n$. \\ \\
When are RBFs cheaper to test? \\
\blu{Answer:} From the above answers, we can see that for any n>1, Gaussian RBFs will always cost more than testing under linear basis.


\section{Very-Short Answer Questions}

\subsection{Essentials}
\rubric{reasoning:10}

\enum{
\item In regression, why do we compute the squared error $(y_i - \hat{y}_i)^2$ rather than testing the equality $(y_i = \hat{y}_i)$? \\
\blu{Answer:} Since we are looking at continuous data, the probability of finding an exact match is close to zero. So it is better to compute the squared error and try to minimize it.
\item Describe a situation in which the least squares estimate would not be unique when $d=2$ and $n=4$. \\
\blu{Answer:} The linear squares estimate would not be unique when the columns of the matrix are identical, i.e they are collinear.
\item What is the computational complexity of computing the closed-form (exact) solution to a linear least squares problem where we have one feature ($d = 1$) and use polynomial basis of degree $p$? \\
\blu{Answer:} The computational complexity of finding the closed form solution is $O(n^3 + np)$.
\item In what circumstance would a regression tree with linear regressions at the leaves be a better choice
than a linear least squares regression model? \\
\blu{Answer:} A regression tree with linear regressions at the leaves be a better choice than a linear least squares regression mode when the data given is discrete.
\item When fitting a model, why do we care if our loss function is convex? \\
\blu{Answer:} Because in a convex function, a local minima is global minima so it becomes easy to find the minimum.
\item When should we consider using gradient descent to approximate the solution to the least squares problem instead of exactly solving it with the closed form solution? \\
\blu{Answer:} The computational cost for least squares problem is $O(nd^2 + d^3)$  so in case of large number of features, i.e. a large $d$, the cost becomes very high. In this case, we should consider using gradient descent instead.
\item Why is optimization non-trivial? Can't we just set the gradient to zero and be done immediately? \\
\blu{Answer:} Optimization might be non-trivial for quite a few reasons. Some of them are: the equation might not always be easy to solve, i.e. finding the gradient might be computationally hard. Also, closed form solution might not be available. Additionally, the equation might not even be differentiable.
\item Why do we need gradient descent for the robust regression problem, as opposed to just using the normal equations? Hint: it is NOT because of the non-differentiability. Recall that we used gradient descent even after smoothing away the non-differentiable part of the loss. \\
\blu{Answer:} We need gradient descent for the robust regression problem as opposed to normal equations because the normal equations can give misleading results if their underlying assumptions are not true, which is generally the case with actual data.
\item What is the problem with having too small of a learning rate in gradient descent? \\
\blu{Answer:} If the learning rate is very small, we will need too many iterations in order to converge.
\item What is the problem with having too large of a learning rate in gradient descent? \\
\blu{Answer:} If the learning rate is too large, it is quite possible that the optimal solution will be skipped together and convergence might not happen.
}

\subsection{These ones are optional and not for marks}

\enum{
\item In \texttt{LinearModelGradient} there's code that checks your gradient using \texttt{scipy.optimize.approx\_fprime}. But, wait a minute: if we can check the gradient, that means we already have it. So, why do we even bother taking the gradient by hand? 
\item What would go wrong if we tried to apply gradient descent to the un-smoothed absolute value loss?
}


\end{document}
